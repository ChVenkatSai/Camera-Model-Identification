{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\n\n# datagen = ImageDataGenerator(rescale=1./255)\n# train_generator = datagen.flow_from_directory(\n#         '../input/sp-society-camera-model-identification/train/train',  batch_size=1,\n#         class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:44:29.70143Z","iopub.execute_input":"2021-05-29T10:44:29.70214Z","iopub.status.idle":"2021-05-29T10:44:29.708027Z","shell.execute_reply.started":"2021-05-29T10:44:29.702075Z","shell.execute_reply":"2021-05-29T10:44:29.706885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# # let's have a look at the images\n# x, y = train_generator.next()\n# plt.imshow((x[0]*255).astype('uint8'));\n# print(list(train_generator.class_indices.keys())[np.argmax(y)])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:44:30.401935Z","iopub.execute_input":"2021-05-29T10:44:30.402486Z","iopub.status.idle":"2021-05-29T10:44:30.406036Z","shell.execute_reply.started":"2021-05-29T10:44:30.402454Z","shell.execute_reply":"2021-05-29T10:44:30.405162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n\n\n# X_data, Y_data = [], []\n# for _ in tqdm(range(2750)):\n#     x, y = train_generator.next()\n#     X_data.append(x[0])\n#     Y_data.append(y[0])\n# X_data = np.asarray(X_data)\n# Y_data = np.asarray(Y_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:44:32.657508Z","iopub.execute_input":"2021-05-29T10:44:32.658156Z","iopub.status.idle":"2021-05-29T10:44:32.662432Z","shell.execute_reply.started":"2021-05-29T10:44:32.658117Z","shell.execute_reply":"2021-05-29T10:44:32.661252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_test = []\n# sub = pd.read_csv('../input/sp-society-camera-model-identification/sample_submission.csv')\n\n# for fname in tqdm(sub['fname']):\n#     filepath = '../input/sp-society-camera-model-identification/test/test/' + fname\n#     X_test.append(img_to_array(load_img(filepath, target_size=(256, 256))))\n# X_test = np.asarray(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:44:35.008982Z","iopub.execute_input":"2021-05-29T10:44:35.009444Z","iopub.status.idle":"2021-05-29T10:44:35.013869Z","shell.execute_reply.started":"2021-05-29T10:44:35.009401Z","shell.execute_reply":"2021-05-29T10:44:35.012495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import Pool, cpu_count\nfrom PIL import Image\n\nfrom glob import glob\nimport os\nimport numpy as np\nimport pywt\nfrom numpy.fft import fft2, ifft2\nfrom scipy.ndimage import filters\nfrom sklearn.metrics import roc_curve, auc\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\n\n\nclass ArgumentError(Exception):\n    pass\n\n\n\"\"\"\nExtraction functions\n\"\"\"\n\n\ndef extract_single(im: np.ndarray,\n                   levels: int = 4,\n                   sigma: float = 5,\n                   wdft_sigma: float = 0) -> np.ndarray:\n    \"\"\"\n    Extract noise residual from a single image\n    :param im: grayscale or color image, np.uint8\n    :param levels: number of wavelet decomposition levels\n    :param sigma: estimated noise power\n    :param wdft_sigma: estimated DFT noise power\n    :return: noise residual\n    \"\"\"\n\n    W = noise_extract(im, levels, sigma)\n    W = rgb2gray(W)\n    W = zero_mean_total(W)\n    W_std = W.std(ddof=1) if wdft_sigma == 0 else wdft_sigma\n    W = wiener_dft(W, W_std).astype(np.float32)\n\n    return W\n\n\ndef noise_extract(im: np.ndarray, levels: int = 4, sigma: float = 5) -> np.ndarray:\n    \"\"\"\n    NoiseExtract as from Binghamton toolbox.\n    :param im: grayscale or color image, np.uint8\n    :param levels: number of wavelet decomposition levels\n    :param sigma: estimated noise power\n    :return: noise residual\n    \"\"\"\n\n    assert (im.dtype == np.uint8)\n    assert (im.ndim in [2, 3])\n\n    im = im.astype(np.float32)\n\n    noise_var = sigma ** 2\n\n    if im.ndim == 2:\n        im.shape += (1,)\n\n    W = np.zeros(im.shape, np.float32)\n\n    for ch in range(im.shape[2]):\n\n        wlet = None\n        while wlet is None and levels > 0:\n            try:\n                wlet = pywt.wavedec2(im[:, :, ch], 'db4', level=levels)\n            except ValueError:\n                levels -= 1\n                wlet = None\n        if wlet is None:\n            raise ValueError('Impossible to compute Wavelet filtering for input size: {}'.format(im.shape))\n\n        wlet_details = wlet[1:]\n\n        wlet_details_filter = [None] * len(wlet_details)\n        # Cycle over Wavelet levels 1:levels-1\n        for wlet_level_idx, wlet_level in enumerate(wlet_details):\n            # Cycle over H,V,D components\n            level_coeff_filt = [None] * 3\n            for wlet_coeff_idx, wlet_coeff in enumerate(wlet_level):\n                level_coeff_filt[wlet_coeff_idx] = wiener_adaptive(wlet_coeff, noise_var)\n            wlet_details_filter[wlet_level_idx] = tuple(level_coeff_filt)\n\n        # Set filtered detail coefficients for Levels > 0 ---\n        wlet[1:] = wlet_details_filter\n\n        # Set to 0 all Level 0 approximation coefficients ---\n        wlet[0][...] = 0\n\n        # Invert wavelet transform ---\n        wrec = pywt.waverec2(wlet, 'db4')\n        try:\n            W[:, :, ch] = wrec\n        except ValueError:\n            W = np.zeros(wrec.shape[:2] + (im.shape[2],), np.float32)\n            W[:, :, ch] = wrec\n\n    if W.shape[2] == 1:\n        W.shape = W.shape[:2]\n\n    W = W[:im.shape[0], :im.shape[1]]\n\n    return W\n\n\ndef noise_extract_compact(args):\n    \"\"\"\n    Extract residual, multiplied by the image. Useful to save memory in multiprocessing operations\n    :param args: (im, levels, sigma), see noise_extract for usage\n    :return: residual, multiplied by the image\n    \"\"\"\n    w = noise_extract(*args)\n    im = args[0]\n    return (w * im / 255.).astype(np.float32)\n\n\ndef extract_multiple_aligned(imgs: list, levels: int = 4, sigma: float = 5, processes: int = None,\n                             batch_size=cpu_count(), tqdm_str: str = '') -> np.ndarray:\n    \"\"\"\n    Extract PRNU from a list of images. Images are supposed to be the same size and properly oriented\n    :param tqdm_str: tqdm description (see tqdm documentation)\n    :param batch_size: number of parallel processed images\n    :param processes: number of parallel processes\n    :param imgs: list of images of size (H,W,Ch) and type np.uint8\n    :param levels: number of wavelet decomposition levels\n    :param sigma: estimated noise power\n    :return: PRNU\n    \"\"\"\n    assert (isinstance(imgs[0], np.ndarray))\n    assert (imgs[0].ndim == 3)\n    assert (imgs[0].dtype == np.uint8)\n\n    h, w, ch = imgs[0].shape\n\n    RPsum = np.zeros((h, w, ch), np.float32)\n    NN = np.zeros((h, w, ch), np.float32)\n\n    if processes is None or processes > 1:\n        args_list = []\n        for im in imgs:\n            args_list += [(im, levels, sigma)]\n        pool = Pool(processes=processes)\n\n        for batch_idx0 in tqdm(np.arange(start=0, step=batch_size, stop=len(imgs)), disable=tqdm_str == '',\n                               desc=(tqdm_str + ' (1/2)'), dynamic_ncols=True):\n            nni = pool.map(inten_sat_compact, args_list[batch_idx0:batch_idx0 + batch_size])\n            for ni in nni:\n                NN += ni\n            del nni\n\n        for batch_idx0 in tqdm(np.arange(start=0, step=batch_size, stop=len(imgs)), disable=tqdm_str == '',\n                               desc=(tqdm_str + ' (2/2)'), dynamic_ncols=True):\n            wi_list = pool.map(noise_extract_compact, args_list[batch_idx0:batch_idx0 + batch_size])\n            for wi in wi_list:\n                RPsum += wi\n            del wi_list\n\n        pool.close()\n\n    else:  # Single process\n        for im in tqdm(imgs, disable=tqdm_str is None, desc=tqdm_str, dynamic_ncols=True):\n            RPsum += noise_extract_compact((im, levels, sigma))\n            NN += (inten_scale(im) * saturation(im)) ** 2\n\n    K = RPsum / (NN + 1)\n    K = rgb2gray(K)\n    K = zero_mean_total(K)\n    K = wiener_dft(K, K.std(ddof=1)).astype(np.float32)\n\n    return K\n\n\ndef cut_ctr(array: np.ndarray, sizes: tuple) -> np.ndarray:\n    \"\"\"\n    Cut a multi-dimensional array at its center, according to sizes\n    :param array: multidimensional array\n    :param sizes: tuple of the same length as array.ndim\n    :return: multidimensional array, center cut\n    \"\"\"\n    array = array.copy()\n    if not (array.ndim == len(sizes)):\n        raise ArgumentError('array.ndim must be equal to len(sizes)')\n    for axis in range(array.ndim):\n        axis_target_size = sizes[axis]\n        axis_original_size = array.shape[axis]\n        if axis_target_size > axis_original_size:\n            raise ValueError(\n                'Can\\'t have target size {} for axis {} with original size {}'.format(axis_target_size, axis,\n                                                                                      axis_original_size))\n        elif axis_target_size < axis_original_size:\n            axis_start_idx = (axis_original_size - axis_target_size) // 2\n            axis_end_idx = axis_start_idx + axis_target_size\n            array = np.take(array, np.arange(axis_start_idx, axis_end_idx), axis)\n    return array\n\n\ndef wiener_dft(im: np.ndarray, sigma: float) -> np.ndarray:\n    \"\"\"\n    Adaptive Wiener filter applied to the 2D FFT of the image\n    :param im: multidimensional array\n    :param sigma: estimated noise power\n    :return: filtered version of input im\n    \"\"\"\n    noise_var = sigma ** 2\n    h, w = im.shape\n\n    im_noise_fft = fft2(im)\n    im_noise_fft_mag = np.abs(im_noise_fft / (h * w) ** .5)\n\n    im_noise_fft_mag_noise = wiener_adaptive(im_noise_fft_mag, noise_var)\n\n    zeros_y, zeros_x = np.nonzero(im_noise_fft_mag == 0)\n\n    im_noise_fft_mag[zeros_y, zeros_x] = 1\n    im_noise_fft_mag_noise[zeros_y, zeros_x] = 0\n\n    im_noise_fft_filt = im_noise_fft * im_noise_fft_mag_noise / im_noise_fft_mag\n    im_noise_filt = np.real(ifft2(im_noise_fft_filt))\n\n    return im_noise_filt.astype(np.float32)\n\n\ndef zero_mean(im: np.ndarray) -> np.ndarray:\n    \"\"\"\n    ZeroMean called with the 'both' argument, as from Binghamton toolbox.\n    :param im: multidimensional array\n    :return: zero mean version of input im\n    \"\"\"\n    # Adapt the shape ---\n    if im.ndim == 2:\n        im.shape += (1,)\n\n    h, w, ch = im.shape\n\n    # Subtract the 2D mean from each color channel ---\n    ch_mean = im.mean(axis=0).mean(axis=0)\n    ch_mean.shape = (1, 1, ch)\n    i_zm = im - ch_mean\n\n    # Compute the 1D mean along each row and each column, then subtract ---\n    row_mean = i_zm.mean(axis=1)\n    col_mean = i_zm.mean(axis=0)\n\n    row_mean.shape = (h, 1, ch)\n    col_mean.shape = (1, w, ch)\n\n    i_zm_r = i_zm - row_mean\n    i_zm_rc = i_zm_r - col_mean\n\n    # Restore the shape ---\n    if im.shape[2] == 1:\n        i_zm_rc.shape = im.shape[:2]\n\n    return i_zm_rc\n\n\ndef zero_mean_total(im: np.ndarray) -> np.ndarray:\n    \"\"\"\n    ZeroMeanTotal as from Binghamton toolbox.\n    :param im: multidimensional array\n    :return: zero mean version of input im\n    \"\"\"\n    im[0::2, 0::2] = zero_mean(im[0::2, 0::2])\n    im[1::2, 0::2] = zero_mean(im[1::2, 0::2])\n    im[0::2, 1::2] = zero_mean(im[0::2, 1::2])\n    im[1::2, 1::2] = zero_mean(im[1::2, 1::2])\n    return im\n\n\ndef rgb2gray(im: np.ndarray) -> np.ndarray:\n    \"\"\"\n    RGB to gray as from Binghamton toolbox.\n    :param im: multidimensional array\n    :return: grayscale version of input im\n    \"\"\"\n    rgb2gray_vector = np.asarray([0.29893602, 0.58704307, 0.11402090]).astype(np.float32)\n    rgb2gray_vector.shape = (3, 1)\n\n    if im.ndim == 2:\n        im_gray = np.copy(im)\n    elif im.shape[2] == 1:\n        im_gray = np.copy(im[:, :, 0])\n    elif im.shape[2] == 3:\n        w, h = im.shape[:2]\n        im = np.reshape(im, (w * h, 3))\n        im_gray = np.dot(im, rgb2gray_vector)\n        im_gray.shape = (w, h)\n    else:\n        raise ValueError('Input image must have 1 or 3 channels')\n\n    return im_gray.astype(np.float32)\n\n\ndef threshold(wlet_coeff_energy_avg: np.ndarray, noise_var: float) -> np.ndarray:\n    \"\"\"\n    Noise variance theshold as from Binghamton toolbox.\n    :param wlet_coeff_energy_avg:\n    :param noise_var:\n    :return: noise variance threshold\n    \"\"\"\n    res = wlet_coeff_energy_avg - noise_var\n    return (res + np.abs(res)) / 2\n\n\ndef wiener_adaptive(x: np.ndarray, noise_var: float, **kwargs) -> np.ndarray:\n    \"\"\"\n    WaveNoise as from Binghamton toolbox.\n    Wiener adaptive flter aimed at extracting the noise component\n    For each input pixel the average variance over a neighborhoods of different window sizes is first computed.\n    The smaller average variance is taken into account when filtering according to Wiener.\n    :param x: 2D matrix\n    :param noise_var: Power spectral density of the noise we wish to extract (S)\n    :param window_size_list: list of window sizes\n    :return: wiener filtered version of input x\n    \"\"\"\n    window_size_list = list(kwargs.pop('window_size_list', [3, 5, 7, 9]))\n\n    energy = x ** 2\n\n    avg_win_energy = np.zeros(x.shape + (len(window_size_list),))\n    for window_idx, window_size in enumerate(window_size_list):\n        avg_win_energy[:, :, window_idx] = filters.uniform_filter(energy,\n                                                                  window_size,\n                                                                  mode='constant')\n\n    coef_var = threshold(avg_win_energy, noise_var)\n    coef_var_min = np.min(coef_var, axis=2)\n\n    x = x * noise_var / (coef_var_min + noise_var)\n\n    return x\n\n\ndef inten_scale(im: np.ndarray) -> np.ndarray:\n    \"\"\"\n    IntenScale as from Binghamton toolbox\n    :param im: type np.uint8\n    :return: intensity scaled version of input x\n    \"\"\"\n\n    assert (im.dtype == np.uint8)\n\n    T = 252\n    v = 6\n    out = np.exp(-1 * (im - T) ** 2 / v)\n    out[im < T] = im[im < T] / T\n\n    return out\n\n\ndef saturation(im: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Saturation as from Binghamton toolbox\n    :param im: type np.uint8\n    :return: saturation map from input im\n    \"\"\"\n    assert (im.dtype == np.uint8)\n\n    if im.ndim == 2:\n        im.shape += (1,)\n\n    h, w, ch = im.shape\n\n    if im.max() < 250:\n        return np.ones((h, w, ch))\n\n    im_h = im - np.roll(im, (0, 1), (0, 1))\n    im_v = im - np.roll(im, (1, 0), (0, 1))\n    satur_map = \\\n        np.bitwise_not(\n            np.bitwise_and(\n                np.bitwise_and(\n                    np.bitwise_and(\n                        im_h != 0, im_v != 0\n                    ), np.roll(im_h, (0, -1), (0, 1)) != 0\n                ), np.roll(im_v, (-1, 0), (0, 1)) != 0\n            )\n        )\n\n    max_ch = im.max(axis=0).max(axis=0)\n\n    for ch_idx, max_c in enumerate(max_ch):\n        if max_c > 250:\n            satur_map[:, :, ch_idx] = \\\n                np.bitwise_not(\n                    np.bitwise_and(\n                        im[:, :, ch_idx] == max_c, satur_map[:, :, ch_idx]\n                    )\n                )\n\n    return satur_map\n\n\ndef inten_sat_compact(args):\n    \"\"\"\n    Memory saving version of inten_scale followed by saturation. Useful for multiprocessing\n    :param args:\n    :return: intensity scale and saturation of input\n    \"\"\"\n    im = args[0]\n    return ((inten_scale(im) * saturation(im)) ** 2).astype(np.float32)\n\n\n\"\"\"\nCross-correlation functions\n\"\"\"\n\n\ndef crosscorr_2d(k1: np.ndarray, k2: np.ndarray) -> np.ndarray:\n    \"\"\"\n    PRNU 2D cross-correlation\n    :param k1: 2D matrix of size (h1,w1)\n    :param k2: 2D matrix of size (h2,w2)\n    :return: 2D matrix of size (max(h1,h2),max(w1,w2))\n    \"\"\"\n    assert (k1.ndim == 2)\n    assert (k2.ndim == 2)\n\n    max_height = max(k1.shape[0], k2.shape[0])\n    max_width = max(k1.shape[1], k2.shape[1])\n\n    k1 -= k1.flatten().mean()\n    k2 -= k2.flatten().mean()\n\n    k1 = np.pad(k1, [(0, max_height - k1.shape[0]), (0, max_width - k1.shape[1])], mode='constant', constant_values=0)\n    k2 = np.pad(k2, [(0, max_height - k2.shape[0]), (0, max_width - k2.shape[1])], mode='constant', constant_values=0)\n\n    k1_fft = fft2(k1, )\n    k2_fft = fft2(np.rot90(k2, 2), )\n\n    return np.real(ifft2(k1_fft * k2_fft)).astype(np.float32)\n\n\ndef aligned_cc(k1: np.ndarray, k2: np.ndarray) -> dict:\n    \"\"\"\n    Aligned PRNU cross-correlation\n    :param k1: (n1,nk) or (n1,nk1,nk2,...)\n    :param k2: (n2,nk) or (n2,nk1,nk2,...)\n    :return: {'cc':(n1,n2) cross-correlation matrix,'ncc':(n1,n2) normalized cross-correlation matrix}\n    \"\"\"\n\n    # Type cast\n    k1 = np.array(k1).astype(np.float32)\n    k2 = np.array(k2).astype(np.float32)\n\n    ndim1 = k1.ndim\n    ndim2 = k2.ndim\n    assert (ndim1 == ndim2)\n\n    k1 = np.ascontiguousarray(k1).reshape(k1.shape[0], -1)\n    k2 = np.ascontiguousarray(k2).reshape(k2.shape[0], -1)\n\n    assert (k1.shape[1] == k2.shape[1])\n\n    k1_norm = np.linalg.norm(k1, ord=2, axis=1, keepdims=True)\n    k2_norm = np.linalg.norm(k2, ord=2, axis=1, keepdims=True)\n\n    k2t = np.ascontiguousarray(k2.transpose())\n\n    cc = np.matmul(k1, k2t).astype(np.float32)\n    ncc = (cc / (k1_norm * k2_norm.transpose())).astype(np.float32)\n\n    return {'cc': cc, 'ncc': ncc}\n\n\ndef pce(cc: np.ndarray, neigh_radius: int = 2) -> dict:\n    \"\"\"\n    PCE position and value\n    :param cc: as from crosscorr2d\n    :param neigh_radius: radius around the peak to be ignored while computing floor energy\n    :return: {'peak':(y,x), 'pce': peak to floor ratio, 'cc': cross-correlation value at peak position\n    \"\"\"\n    assert (cc.ndim == 2)\n    assert (isinstance(neigh_radius, int))\n\n    out = dict()\n\n    max_idx = np.argmax(cc.flatten())\n    max_y, max_x = np.unravel_index(max_idx, cc.shape)\n\n    peak_height = cc[max_y, max_x]\n\n    cc_nopeaks = cc.copy()\n    cc_nopeaks[max_y - neigh_radius:max_y + neigh_radius, max_x - neigh_radius:max_x + neigh_radius] = 0\n\n    pce_energy = np.mean(cc_nopeaks.flatten() ** 2)\n\n    out['peak'] = (max_y, max_x)\n    out['pce'] = (peak_height ** 2) / pce_energy * np.sign(peak_height)\n    out['cc'] = peak_height\n\n    return out\n\n\n\"\"\"\nStatistical functions\n\"\"\"\n\n\ndef stats(cc: np.ndarray, gt: np.ndarray, ) -> dict:\n    \"\"\"\n    Compute statistics\n    :param cc: cross-correlation or normalized cross-correlation matrix\n    :param gt: boolean multidimensional array representing groundtruth\n    :return: statistics dictionary\n    \"\"\"\n    assert (cc.shape == gt.shape)\n    assert (gt.dtype == np.bool)\n\n    assert (cc.shape == gt.shape)\n    assert (gt.dtype == np.bool)\n\n    fpr, tpr, th = roc_curve(gt.flatten(), cc.flatten())\n    auc_score = auc(fpr, tpr)\n\n    # EER\n    eer_idx = np.argmin((fpr - (1 - tpr)) ** 2, axis=0)\n    eer = float(fpr[eer_idx])\n\n    outdict = {\n        'tpr': tpr,\n        'fpr': fpr,\n        'th': th,\n        'auc': auc_score,\n        'eer': eer,\n    }\n\n    return outdict\n\n\ndef gt(l1: list or np.ndarray, l2: list or np.ndarray) -> np.ndarray:\n    \"\"\"\n    Determine the Ground Truth matrix given the labels\n    :param l1: fingerprints labels\n    :param l2: residuals labels\n    :return: groundtruth matrix\n    \"\"\"\n    l1 = np.array(l1)\n    l2 = np.array(l2)\n\n    assert (l1.ndim == 1)\n    assert (l2.ndim == 1)\n\n    gt_arr = np.zeros((len(l1), len(l2)), np.bool)\n\n    for l1idx, l1sample in enumerate(l1):\n        gt_arr[l1idx, l2 == l1sample] = True\n\n    return gt_arr\n\n#img=cv2.imread('chilis.jpg')\n\n#plt.imshow(noise_extract(img))\n#plt.imshow(extract_multiple_aligned([img]))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T12:17:48.238343Z","iopub.execute_input":"2021-05-29T12:17:48.23868Z","iopub.status.idle":"2021-05-29T12:17:49.01201Z","shell.execute_reply.started":"2021-05-29T12:17:48.238649Z","shell.execute_reply":"2021-05-29T12:17:49.011073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nff_dirlist = np.array(sorted(glob('../input/sp-society-camera-model-identification/train/train/*')))\nff_device = np.array([os.path.split(i)[1].rsplit('_', 1)[0] for i in ff_dirlist])\nfingerprint_device = sorted(np.unique(ff_device))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T12:17:50.605923Z","iopub.execute_input":"2021-05-29T12:17:50.606249Z","iopub.status.idle":"2021-05-29T12:17:50.61375Z","shell.execute_reply.started":"2021-05-29T12:17:50.606219Z","shell.execute_reply":"2021-05-29T12:17:50.612872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# k = []\n# for device in fingerprint_device:\n#         imgs = []\n#         for img_dir in ff_dirlist[ff_device == device]:\n#             images = glob(img_dir+'/*')\n#             for img_path in images:\n#                 im = Image.open(img_path)\n#                 im_arr = np.asarray(im)\n#                 if im_arr.dtype != np.uint8:\n#                     print('Error while reading image: {}'.format(img_path))\n#                     continue\n#                 if im_arr.ndim != 3:\n#                     print('Image is not RGB: {}'.format(img_path))\n#                     continue\n#                 im_cut = cut_ctr(im_arr, (512, 512, 3))\n#                 imgs += [im_cut]\n#         k += [extract_multiple_aligned(imgs, processes=cpu_count())]\n# k = np.stack(k, 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T12:18:02.735872Z","iopub.execute_input":"2021-05-29T12:18:02.736196Z","iopub.status.idle":"2021-05-29T12:18:02.739659Z","shell.execute_reply.started":"2021-05-29T12:18:02.736168Z","shell.execute_reply":"2021-05-29T12:18:02.738833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = []\nmodels = []\nfor device in fingerprint_device:\n        #imgs=[]\n        for img_dir in ff_dirlist[ff_device == device]:\n            images = glob(img_dir+'/*')\n            for img_path in images:\n                im = Image.open(img_path)\n                im_arr = np.asarray(im)\n                if im_arr.dtype != np.uint8:\n                    print('Error while reading image: {}'.format(img_path))\n                    continue\n                if im_arr.ndim != 3:\n                    print('Image is not RGB: {}'.format(img_path))\n                    continue\n                im_cut = cut_ctr(im_arr, (512, 512, 3))\n                rgb=cv2.cvtColor(extract_single(im_cut),cv2.COLOR_GRAY2RGB)\n                rgb_in=cv2.resize(rgb,(224,224))\n                imgs.append(rgb_in)\n                models.append(list(ff_device==device).index(True))\n        #imgs_and_models.append(imgs)     ","metadata":{"execution":{"iopub.status.busy":"2021-05-29T12:18:03.589248Z","iopub.execute_input":"2021-05-29T12:18:03.589561Z","iopub.status.idle":"2021-05-29T12:41:16.96144Z","shell.execute_reply.started":"2021-05-29T12:18:03.589531Z","shell.execute_reply":"2021-05-29T12:41:16.960473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imgs = []\n#     for img_path in nat_dirlist:\n#         imgs += [prnu.cut_ctr(np.asarray(Image.open(img_path)), (512, 512, 3))]\n\n#     pool = Pool(cpu_count())\n#     w = pool.map(prnu.extract_single, imgs)\n#     pool.close()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T19:45:04.342527Z","iopub.execute_input":"2021-05-28T19:45:04.342932Z","iopub.status.idle":"2021-05-28T19:45:04.569723Z","shell.execute_reply.started":"2021-05-28T19:45:04.342892Z","shell.execute_reply":"2021-05-28T19:45:04.564352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.densenet import DenseNet201\nfrom keras.applications.densenet import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\n# re-size all the images to this\nIMAGE_SIZE = [224,224]\n\ntrain_path = '/kaggle/input/sp-society-camera-model-identification/train/train'\nvalid_path = '/kaggle/input/sp-society-camera-model-identification/test/test'\n\n# add preprocessing layer to the front of VGG\ndensenet = DenseNet201(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n\n# don't train existing weights\nfor layer in densenet.layers:\n  layer.trainable = False\n  \nprint(densenet.output)\n  \n  # useful for getting number of classes\nfolders = glob('/kaggle/input/sp-society-camera-model-identification/train/train/*')\n\nprint(folders)\n# our layers - you can add more if you want\nx = Flatten()(densenet.output)\n# x = Dense(1000, activation='relu')(x)\nprediction = Dense(len(folders), activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=densenet.input, outputs=prediction)\n\n# view the structure of the model\nmodel.summary()\n\n# tell the model what cost and optimization method to use\nmodel.compile(\n  loss='sparse_categorical_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)\n#import tensorflow as tf\n#X_train=tf.convert_to_tensor(X_train)\n#Y_train=tf.convert_to_tensor(Y_train,dtype='float32')\n#det1 = pd.DataFrame(inp_train)\n#det2 = pd.DataFrame(out_train)\n#det = pd.concat([det1,det2],join='outer',axis=1)\nX_train=np.array(imgs)\nY_train=np.array(models)\n\nfrom sklearn.model_selection import train_test_split\ninp_train,inp_test,out_train,out_test = train_test_split(X_train,Y_train,test_size = 0.2)\n\n\nr=model.fit(inp_train,out_train,validation_data=(inp_test,out_test),epochs=100,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T12:50:49.113758Z","iopub.execute_input":"2021-05-29T12:50:49.114129Z","iopub.status.idle":"2021-05-29T13:03:55.800435Z","shell.execute_reply.started":"2021-05-29T12:50:49.114097Z","shell.execute_reply":"2021-05-29T13:03:55.799584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv2.resize(X_train[0],(224,224))\nplt.imshow(X_train[0])\ntype(X_train[0]),X_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-29T07:24:16.256883Z","iopub.execute_input":"2021-05-29T07:24:16.257474Z","iopub.status.idle":"2021-05-29T07:24:16.432928Z","shell.execute_reply.started":"2021-05-29T07:24:16.257427Z","shell.execute_reply":"2021-05-29T07:24:16.432106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(imgs).reshape((2750,(224,224,3)))\nY_train=np.array(models).astype('float32')\n\nY_train.shape,X_train.shape\n#plt.imshow(X_train[2])\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T10:27:10.027132Z","iopub.execute_input":"2021-05-29T10:27:10.027496Z","iopub.status.idle":"2021-05-29T10:27:10.048997Z","shell.execute_reply.started":"2021-05-29T10:27:10.027466Z","shell.execute_reply":"2021-05-29T10:27:10.047519Z"},"trusted":true},"execution_count":null,"outputs":[]}]}